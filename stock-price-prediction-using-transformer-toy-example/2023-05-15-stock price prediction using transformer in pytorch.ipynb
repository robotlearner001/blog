{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a simple example to define transformer in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1493, -1.3924, -0.4691,  ...,  1.3503,  0.0981,  0.5888],\n",
       "         [-2.2525, -0.6865,  0.3882,  ...,  0.0939, -0.7536, -0.2908]],\n",
       "\n",
       "        [[-1.0448, -1.9382,  0.3898,  ...,  1.5998, -0.3502,  0.0884],\n",
       "         [-1.3929, -0.1478, -0.3485,  ...,  0.6100, -0.9397,  0.3129]],\n",
       "\n",
       "        [[-0.0511, -0.9572, -0.3308,  ...,  1.1702,  0.1238,  0.1381],\n",
       "         [-1.5637, -0.3723, -0.0174,  ...,  0.9533, -1.2683,  0.2479]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.9024, -0.7733,  0.1455,  ...,  0.7555, -1.4278,  0.4268],\n",
       "         [-0.6650, -0.2154, -0.2301,  ...,  0.2651, -1.4691,  0.0540]],\n",
       "\n",
       "        [[-0.1727, -0.9760,  0.7051,  ...,  0.5820, -0.9048,  0.1648],\n",
       "         [-1.2072, -0.7008,  0.0726,  ...,  0.3305, -1.2804, -0.8328]],\n",
       "\n",
       "        [[-0.3538, -1.7924, -0.7605,  ...,  0.6110, -0.1426, -0.1498],\n",
       "         [-0.9802,  0.1632, -0.3538,  ...,  0.6591, -0.2645, -0.1160]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the hyperparameters\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_layers = 6\n",
    "dropout = 0.1\n",
    "\n",
    "# Create the Transformer model\n",
    "transformer = nn.Transformer(d_model, nhead, num_layers, dropout=dropout)\n",
    "\n",
    "# Example input data (batch_size=2, seq_length=10, d_model=512)\n",
    "src = torch.randn(10, 2, 512)\n",
    "tgt = torch.randn(10, 2, 512)\n",
    "\n",
    "# Pass the input data through the Transformer model\n",
    "output = transformer(src, tgt)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stock price example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 3398.44091796875\n",
      "Epoch 2/100, Loss: 3316.48193359375\n",
      "Epoch 3/100, Loss: 3222.744140625\n",
      "Epoch 4/100, Loss: 3129.048828125\n",
      "Epoch 5/100, Loss: 3029.943359375\n",
      "Epoch 6/100, Loss: 2930.379150390625\n",
      "Epoch 7/100, Loss: 2826.78173828125\n",
      "Epoch 8/100, Loss: 2720.460693359375\n",
      "Epoch 9/100, Loss: 2614.76806640625\n",
      "Epoch 10/100, Loss: 2508.346435546875\n",
      "Epoch 11/100, Loss: 2402.098388671875\n",
      "Epoch 12/100, Loss: 2296.591552734375\n",
      "Epoch 13/100, Loss: 2194.035400390625\n",
      "Epoch 14/100, Loss: 2093.64111328125\n",
      "Epoch 15/100, Loss: 1995.6507568359375\n",
      "Epoch 16/100, Loss: 1901.906005859375\n",
      "Epoch 17/100, Loss: 1812.193359375\n",
      "Epoch 18/100, Loss: 1727.382568359375\n",
      "Epoch 19/100, Loss: 1646.742919921875\n",
      "Epoch 20/100, Loss: 1572.1201171875\n",
      "Epoch 21/100, Loss: 1502.7958984375\n",
      "Epoch 22/100, Loss: 1437.83447265625\n",
      "Epoch 23/100, Loss: 1379.832763671875\n",
      "Epoch 24/100, Loss: 1326.247314453125\n",
      "Epoch 25/100, Loss: 1278.2840576171875\n",
      "Epoch 26/100, Loss: 1235.185546875\n",
      "Epoch 27/100, Loss: 1197.093994140625\n",
      "Epoch 28/100, Loss: 1163.26611328125\n",
      "Epoch 29/100, Loss: 1134.0562744140625\n",
      "Epoch 30/100, Loss: 1109.780517578125\n",
      "Epoch 31/100, Loss: 1086.330810546875\n",
      "Epoch 32/100, Loss: 1066.9732666015625\n",
      "Epoch 33/100, Loss: 1050.01123046875\n",
      "Epoch 34/100, Loss: 1035.6026611328125\n",
      "Epoch 35/100, Loss: 1024.16064453125\n",
      "Epoch 36/100, Loss: 1013.3431396484375\n",
      "Epoch 37/100, Loss: 1004.4152221679688\n",
      "Epoch 38/100, Loss: 997.2017822265625\n",
      "Epoch 39/100, Loss: 991.1550903320312\n",
      "Epoch 40/100, Loss: 986.3880615234375\n",
      "Epoch 41/100, Loss: 982.2947387695312\n",
      "Epoch 42/100, Loss: 979.56005859375\n",
      "Epoch 43/100, Loss: 975.6256103515625\n",
      "Epoch 44/100, Loss: 972.8544921875\n",
      "Epoch 45/100, Loss: 971.7625122070312\n",
      "Epoch 46/100, Loss: 970.3599853515625\n",
      "Epoch 47/100, Loss: 970.59521484375\n",
      "Epoch 48/100, Loss: 966.77978515625\n",
      "Epoch 49/100, Loss: 965.78515625\n",
      "Epoch 50/100, Loss: 966.2213745117188\n",
      "Epoch 51/100, Loss: 966.7298583984375\n",
      "Epoch 52/100, Loss: 965.7799682617188\n",
      "Epoch 53/100, Loss: 964.7244262695312\n",
      "Epoch 54/100, Loss: 965.6656494140625\n",
      "Epoch 55/100, Loss: 962.5753173828125\n",
      "Epoch 56/100, Loss: 965.830078125\n",
      "Epoch 57/100, Loss: 965.7347412109375\n",
      "Epoch 58/100, Loss: 964.0916748046875\n",
      "Epoch 59/100, Loss: 960.5569458007812\n",
      "Epoch 60/100, Loss: 961.8713989257812\n",
      "Epoch 61/100, Loss: 962.349609375\n",
      "Epoch 62/100, Loss: 960.49755859375\n",
      "Epoch 63/100, Loss: 964.2122802734375\n",
      "Epoch 64/100, Loss: 963.5234375\n",
      "Epoch 65/100, Loss: 960.2454833984375\n",
      "Epoch 66/100, Loss: 963.3313598632812\n",
      "Epoch 67/100, Loss: 964.2076416015625\n",
      "Epoch 68/100, Loss: 963.0060424804688\n",
      "Epoch 69/100, Loss: 963.3466796875\n",
      "Epoch 70/100, Loss: 961.5718994140625\n",
      "Epoch 71/100, Loss: 965.1921997070312\n",
      "Epoch 72/100, Loss: 963.89453125\n",
      "Epoch 73/100, Loss: 960.8636474609375\n",
      "Epoch 74/100, Loss: 960.2990112304688\n",
      "Epoch 75/100, Loss: 963.4850463867188\n",
      "Epoch 76/100, Loss: 963.6348876953125\n",
      "Epoch 77/100, Loss: 964.7691650390625\n",
      "Epoch 78/100, Loss: 962.9620361328125\n",
      "Epoch 79/100, Loss: 964.328125\n",
      "Epoch 80/100, Loss: 962.6884765625\n",
      "Epoch 81/100, Loss: 961.5846557617188\n",
      "Epoch 82/100, Loss: 962.5060424804688\n",
      "Epoch 83/100, Loss: 960.41796875\n",
      "Epoch 84/100, Loss: 959.3986206054688\n",
      "Epoch 85/100, Loss: 962.3971557617188\n",
      "Epoch 86/100, Loss: 961.201171875\n",
      "Epoch 87/100, Loss: 965.5858154296875\n",
      "Epoch 88/100, Loss: 965.72802734375\n",
      "Epoch 89/100, Loss: 956.4362182617188\n",
      "Epoch 90/100, Loss: 968.4918212890625\n",
      "Epoch 91/100, Loss: 963.67041015625\n",
      "Epoch 92/100, Loss: 964.4407958984375\n",
      "Epoch 93/100, Loss: 959.97216796875\n",
      "Epoch 94/100, Loss: 966.7526245117188\n",
      "Epoch 95/100, Loss: 962.8594970703125\n",
      "Epoch 96/100, Loss: 960.51611328125\n",
      "Epoch 97/100, Loss: 964.47314453125\n",
      "Epoch 98/100, Loss: 966.1231689453125\n",
      "Epoch 99/100, Loss: 962.8785400390625\n",
      "Epoch 100/100, Loss: 961.90283203125\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Generate dummy stock price data\n",
    "num_days = 200\n",
    "stock_prices = np.random.rand(num_days) * 100\n",
    "\n",
    "# Preprocess the data\n",
    "input_seq_len = 10\n",
    "output_seq_len = 5\n",
    "num_samples = num_days - input_seq_len - output_seq_len + 1\n",
    "\n",
    "src_data = torch.tensor([stock_prices[i:i+input_seq_len] for i in range(num_samples)]).unsqueeze(-1).float()\n",
    "tgt_data = torch.tensor([stock_prices[i+input_seq_len:i+input_seq_len+output_seq_len] for i in range(num_samples)]).unsqueeze(-1).float()\n",
    "\n",
    "# Create a custom Transformer model\n",
    "class StockPriceTransformer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dropout):\n",
    "        super(StockPriceTransformer, self).__init__()\n",
    "        self.input_linear = nn.Linear(1, d_model)\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_layers, dropout=dropout)\n",
    "        self.output_linear = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.input_linear(src)\n",
    "        tgt = self.input_linear(tgt)\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.output_linear(output)\n",
    "        return output\n",
    "\n",
    "d_model = 64\n",
    "nhead = 4\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "\n",
    "model = StockPriceTransformer(d_model, nhead, num_layers, dropout=dropout)\n",
    "\n",
    "# Training parameters\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "batch_size = 16\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        src_batch = src_data[i:i+batch_size].transpose(0, 1)\n",
    "        tgt_batch = tgt_data[i:i+batch_size].transpose(0, 1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src_batch, tgt_batch[:-1])\n",
    "        loss = criterion(output, tgt_batch[1:])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prediction in autogregressive fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next 5 days of stock prices: [51.60749435424805, 50.53663635253906, 50.62848663330078, 51.24516296386719, 50.89353942871094]\n"
     ]
    }
   ],
   "source": [
    "# Predict the next 5 days of stock prices one at a time\n",
    "src = torch.tensor(stock_prices[-input_seq_len:]).unsqueeze(-1).unsqueeze(1).float()\n",
    "tgt = torch.zeros(output_seq_len, 1, 1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(output_seq_len):\n",
    "        prediction = model(src, tgt[:i+1])\n",
    "        tgt[i] = prediction[-1]\n",
    "\n",
    "output = tgt.squeeze().tolist()\n",
    "print(\"Next 5 days of stock prices:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prediction entire output sequence in a single forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next 5 days of stock prices: [51.00934982299805, 51.10614013671875, 51.138946533203125, 50.764610290527344, 50.81841278076172]\n"
     ]
    }
   ],
   "source": [
    "# Predict the next 5 days of stock prices at one time\n",
    "src = torch.tensor(stock_prices[-input_seq_len:]).unsqueeze(-1).unsqueeze(1).float()\n",
    "tgt = torch.zeros(output_seq_len, 1, 1)\n",
    "\n",
    "with torch.no_grad():\n",
    "     prediction = model(src, tgt)\n",
    "\n",
    "output = prediction.squeeze().tolist()\n",
    "print(\"Next 5 days of stock prices:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between `model(src, tgt[:i+1])` and `model(src, tgt)` lies in the target input tensor passed to the model during the prediction loop.\n",
    "\n",
    "1. `model(src, tgt[:i+1])`: In this case, we pass a partial target sequence to the model, which includes the target values up to the current step `i`. This is used in the autoregressive decoding approach, where the model generates the output sequence one step at a time, using its own predictions from previous steps as input for the next steps. This approach is useful when the model needs to generate an output sequence step by step, and the output at each step depends on the previous outputs.\n",
    "\n",
    "2. `model(src, tgt)`: In this case, we pass the entire target sequence to the model at once. This is used when the model is trained to generate the entire output sequence in a single forward pass, given the complete target sequence as input. This approach is useful when the model is trained to generate the output sequence in parallel, and the output at each step does not depend on the previous outputs.\n",
    "\n",
    "In the context of predicting the next 5 days of stock prices at one time, we use the autoregressive decoding approach (`model(src, tgt[:i+1])`) to generate the output sequence step by step, as the output at each step depends on the previous outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
